4.47 has the best performance.<br>
The implementation diverge at the `inner_loop:` part.

## 4.47
```assembly
inner_loop:
    rrmovq  %rcx, %rdx
    addq    %r10, %rdx            # %rdx = curr + 1
    mrmovq    (%rcx), %rax      # %rax = *curr
    mrmovq    8(%rcx), %rsi     # t = *(curr + 1)
    rrmovq  %rsi, %r9
    subq    %rax, %r9           # compare *(curr + 1) with *curr
    jge    inner_loop_next      # if *(curr + 1) >= *curr go to inner_loop_next
    rmmovq    %rax, 8(%rcx)     # *(curr + 1) = *curr
    rmmovq    %rsi, (%rcx)      # *curr = t
    jmp    inner_loop_next      # go to inner_loop_next
```
Ordered array: we'll have n jumps
reversed ordered array: we'll have n jumps, but extra 2*n rmmovq instructions
50% of the cases require swap: n jumps + n rmmovq instructions


## 4.48
```assembly
inner_loop:
    rrmovq  %rcx, %rdx
    addq    %r10, %rdx            # %rdx = curr + 1
    mrmovq    (%rcx), %rax      # %rax = *curr
    mrmovq    8(%rcx), %rsi     # t = *(curr + 1)
    rrmovq  %rsi, %r9
    subq    %rax, %r9           # compare *(curr + 1) with *curr
    cmovl %rax, %r11           # swap %rax and %rsi values
    cmovl %rsi, %rax
    cmovl %r11, %rsi
    rmmovq    %rax, (%rcx)
    rmmovq    %rsi, 8(%rcx)
    jmp    inner_loop_next      # go to inner_loop_next
```
In this implementation, we have 5 instructions running after subq instructions whether swap is required or not.
Compared to 4.47, we have 3 more instructions when swap is required, and 5 more when swap isn't required.
Therefore, this function performance is worse in this case.

## 4.49
```assembly
inner_loop:
    rrmovq  %rcx, %rdx
    addq    %r10, %rdx            # %rdx = curr + 1
    mrmovq    (%rcx), %rax      # %rax = *curr
    mrmovq    8(%rcx), %rsi     # t = *(curr + 1)
    irmovq  $0, %rbx
    rrmovq  %rsi, %r9
    subq    %rax, %r9           # compare *(curr + 1) with *curr
    cmovge %r11, %r12            # %r12 is 1...1 if %r9 < %rax (means *(curr + 1) < *curr) otherwise %r12 equals 0
    rrmovq %r12, %r13
    xorq %r11, %r13             # %r13 = not %r12
    rrmovq %r12, %r9
    rrmovq %r13, %r14
    andq %rax, %r9              # %r9 equals %rax if *(curr + 1) < *curr ;  otherwise %r9 equals 0
    andq %rsi, %r14             # %r14 equals %rsi if *(curr + 1) >= *curr ; otherwise %r14 equals 0
    addq %r14, %r9              # %r9 equals %rax if *(curr + 1) < *curr ; otherwise %r9 equals %rsi
    rmmovq    %r9, (%rcx)
    rrmovq %r12, %r9
    rrmovq %r13, %r14
    andq %rax, %r14             # %r14 equals %rax if *(curr + 1) >= *curr ;  otherwise %r14 equals 0
    andq %rsi, %r9              # %r9 equals %rsi if *(curr + 1) < *curr ; otherwise %r9 equals 0
    addq %r14, %r9              # %r9 equals %rsi if *(curr + 1) < *curr ; otherwise %r9 equals %rax
    rmmovq    %r9, 8(%rcx)
    jmp    inner_loop_next      # go to inner_loop_next
```

Here we run even more instructions than 4.48. Making this the poorest implementation in terms of performance.
