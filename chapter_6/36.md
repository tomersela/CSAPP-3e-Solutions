```c
int x[2][128];
int i;
int sum = 0;

for (i = 0; i < 128; i++) {
    sum += x[0][i] * x[1][i];
}
```

Array x begins at memory address 0x0 and is stored in row-major order.

Therefore, `x[0]` starts at address 0x0 and `x[1]` starts at address `128 * sizeof(int) = 128 * 4 = 512` (`0x200`)

## A
$C = 512\ bytes$

$E = 1$

$B = 16$

$C = S * E * B$

$512 = S * 1 * 16$

$S = 32$


For any i, when 0 < i < 128, x[0][i] and x[1][i] are mapped to the same set (and cache line).

Means, on each iteration, there's a cache miss when x[0][i] is stored in the cache, and then this cache value gets overriden when x[1][i] is loaded.

Therefore, there's 100% miss rate

## B
$C = 1024\ bytes$

$E = 1$

$B = 16$

$C = S * E * B$

$1024 = S * 1 * 16$

$S = 64$

Now both x[0] and x[1] fit the cache (`2 * 128 * sizeof(int) = 128 * 4 = 1024 = C`).

Every 4 integers fit a cache block (B = 16 bytes).

When first integer within a cache block is loaded, the entire block is loaded.

We should expect a 25% miss rate.

## C
$C = 512\ bytes$

$E = 2$

$B = 16$

$C = S * E * B$

$512 = S * 2 * 16$

$S = 8$

As in Case 1, x[0][i] and x[1][i] are mapped to the same set, but since now it's 2-Way associative cache,<br>
x[0][i] and x[1][i] will be mapped to a different cache-lines.

Every 4 integers fit a cache block (B = 16 bytes).

When first integer within a cache block is loaded, the entire block is loaded.

We should expect a 25% miss rate.


## D
A larger cache size won't help to reduce the miss rate since our bottleneck is with the block size.

A block of 16 bytes can only cache 4 integers.<br>
Since each integer is being read only once and in a consecutive manner, we have one miss and 3 hits for every 4 integers within x.

## E
A larger block size will help to reduce the miss rate.<br>
For example, assuming B = 32, each block will be able to fit 8 integers.<br>
This will yield a miss rate of 12.5%
